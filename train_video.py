{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import intrinsics_utils\n",
    "from loss_fn import DMPLoss\n",
    "from depth_prediction_net import DispNetS\n",
    "from object_motion_net import MotionVectorNet\n",
    "\n",
    "rsize_factor = (128,416)\n",
    "\n",
    "class DepthMotionDataset(Dataset):\n",
    "    def __init__(self, mode='train', transform=None, root_dir='./',):\n",
    "        self.image_list = sorted(os.listdir(f'{root_dir}/images/taichung/'))\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_a, img_b = Image.open(f'{self.root_dir}/images/taichung/' + self.image_list[idx]), Image.open(f'{self.root_dir}/images/taichung/' + self.image_list[idx + 1])\n",
    "        if self.transform:\n",
    "            sample_a = self.transform(img_a)\n",
    "            sample_b = self.transform(img_b)\n",
    "        return [sample_a, sample_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, step, depth_net, object_motion_net, motion_field_burning_steps, train=False):\n",
    "    endpoints = {}\n",
    "    rgb_seq_images = x\n",
    "    rgb_images = torch.cat((rgb_seq_images[0], rgb_seq_images[1]), dim=0)\n",
    "    depth_images = depth_net(rgb_images)\n",
    "    depth_seq_images = torch.split(depth_images, depth_images.shape[0] // 2, dim=0)\n",
    "    endpoints['predicted_depth'] = depth_seq_images\n",
    "    endpoints['rgb'] = rgb_seq_images\n",
    "    motion_features = [     \n",
    "            torch.cat((endpoints['rgb'][0], endpoints['predicted_depth'][0]), dim=1),\n",
    "            torch.cat((endpoints['rgb'][1], endpoints['predicted_depth'][1]), dim=1)]\n",
    "    motion_features_stack = torch.cat(motion_features, dim=0)\n",
    "    flipped_motion_features_stack = torch.cat(motion_features[::-1], dim=0)\n",
    "    pairs = torch.cat([motion_features_stack, flipped_motion_features_stack], dim=1)\n",
    "    rot, trans, residual_translation, intrinsics_mat = object_motion_net(pairs)\n",
    "    if train and motion_field_burning_steps > 0.0:\n",
    "        step = base_step * current_epoch\n",
    "        step = torch.tensor(step).type(torch.FloatTensor)\n",
    "        burnin_steps = torch.tensor(motion_field_burning_steps).type(torch.FloatTensor)\n",
    "        residual_translation *= torch.clamp(2 * step / burnin_steps - 1, 0.0, 1.0)\n",
    "    endpoints['residual_translation'] = torch.split(residual_translation, residual_translation.shape[0] // 2, dim=0)\n",
    "    endpoints['background_translation'] = torch.split(trans, trans.shape[0] // 2, dim=0)\n",
    "    endpoints['rotation'] = torch.split(rot, rot.shape[0] // 2, dim=0)\n",
    "    intrinsics_mat = 0.5 * sum(torch.split(intrinsics_mat, intrinsics_mat.shape[0] // 2, dim=0))\n",
    "    endpoints['intrinsics_mat'] = [intrinsics_mat] * 2\n",
    "    endpoints['intrinsics_mat_inv'] = [intrinsics_utils.invert_intrinsics_matrix(intrinsics_mat)] * 2\n",
    "    return endpoints\n",
    "\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "cudnn.deterministic = True\n",
    "PATH = './checkpoints/'\n",
    "\n",
    "default_loss_weights = {'rgb_consistency': 1.0,\n",
    "                        'ssim': 3.0,\n",
    "                        'depth_consistency': 0.05,\n",
    "                        'depth_smoothing': 0.05,\n",
    "                        'rotation_cycle_consistency': 1e-3,\n",
    "                        'translation_cycle_consistency': 5e-2,\n",
    "                        'depth_variance': 0.0,\n",
    "                        'motion_smoothing': 1.0,\n",
    "                        'motion_drift': 0.2,\n",
    "                       }\n",
    "batch_size = 8\n",
    "motion_field_burning_steps = 20000\n",
    "epochs = 90\n",
    "intrinsics_mat = None\n",
    "use_intrinsics = False\n",
    "delete_file = True\n",
    "accumulate_grad_batches = 4\n",
    "metrics = 0\n",
    "\n",
    "train_dataset = DepthMotionDataset(mode='train', transform=transforms.Compose([transforms.Resize(size=rsize_factor),\n",
    "                                                                               transforms.ToTensor(),]),\n",
    "                                   root_dir='./')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=8,\n",
    "                                           drop_last = False,\n",
    "                                           sampler=None,\n",
    "                                           pin_memory=False,\n",
    "                                          )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "depth_net = DispNetS().to(device)\n",
    "object_motion_net = MotionVectorNet(auto_mask=True, intrinsics=use_intrinsics, intrinsics_mat=intrinsics_mat).to(device)\n",
    "loss_func = DMPLoss(default_loss_weights)\n",
    "train_batches = len(train_loader)\n",
    "base_step = (train_batches) // accumulate_grad_batches\n",
    "\n",
    "optimizer = optim.Adam(list(depth_net.parameters()) + list(object_motion_net.parameters()), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for rgb_seq_images in train_loader:\n",
    "        depth_net.train()\n",
    "        object_motion_net.train()\n",
    "        \n",
    "        rgb_seq_images[0], rgb_seq_images[1] = rgb_seq_images[0].to(device), rgb_seq_images[1].to(device)\n",
    "        endpoints = {}\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        rgb_images = torch.cat((rgb_seq_images[0], rgb_seq_images[1]), dim=0)\n",
    "        depth_images = depth_net(rgb_images)\n",
    "        depth_seq_images = torch.split(depth_images, depth_images.shape[0] // 2, dim=0)\n",
    "        endpoints['predicted_depth'] = depth_seq_images\n",
    "        endpoints['rgb'] = rgb_seq_images\n",
    "        motion_features = [torch.cat((endpoints['rgb'][0], endpoints['predicted_depth'][0]), dim=1),\n",
    "                           torch.cat((endpoints['rgb'][1], endpoints['predicted_depth'][1]), dim=1)]\n",
    "        motion_features_stack = torch.cat(motion_features, dim=0)\n",
    "        flipped_motion_features_stack = torch.cat(motion_features[::-1], dim=0)\n",
    "        pairs = torch.cat([motion_features_stack, flipped_motion_features_stack], dim=1)\n",
    "        rot, trans, residual_translation, intrinsics_mat = object_motion_net(pairs)\n",
    "        if motion_field_burning_steps > 0.0:\n",
    "            step = base_step * epoch\n",
    "            step = torch.tensor(step).type(torch.FloatTensor)\n",
    "            burnin_steps = torch.tensor(motion_field_burning_steps).type(torch.FloatTensor)\n",
    "            residual_translation *= torch.clamp(2 * step / burnin_steps - 1, 0.0, 1.0)\n",
    "        endpoints['residual_translation'] = torch.split(residual_translation, residual_translation.shape[0] // 2, dim=0)\n",
    "        endpoints['background_translation'] = torch.split(trans, trans.shape[0] // 2, dim=0)\n",
    "        endpoints['rotation'] = torch.split(rot, rot.shape[0] // 2, dim=0)\n",
    "        intrinsics_mat = 0.5 * sum(torch.split(intrinsics_mat, intrinsics_mat.shape[0] // 2, dim=0))\n",
    "        endpoints['intrinsics_mat'] = [intrinsics_mat] * 2\n",
    "        endpoints['intrinsics_mat_inv'] = [intrinsics_utils.invert_intrinsics_matrix(intrinsics_mat)] * 2\n",
    "        \n",
    "        loss_val = loss_func(endpoints)\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step(metrics)\n",
    "    print(f'Epoch : {epoch:02d}/{epochs}, loss : {loss_val:.03f}')\n",
    "    torch.save(depth_net.state_dict(), PATH)\n",
    "    torch.save(object_motion_net.state_dict(), PATH)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
