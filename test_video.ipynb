{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthMotionLightningModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        \n",
    "        super(DepthMotionLightningModel, self).__init__()\n",
    "        self.default_loss_weights = {\n",
    "                            'rgb_consistency': 1.0,\n",
    "                            'ssim': 3.0,\n",
    "                            'depth_consistency': 0.05,\n",
    "                            'depth_smoothing': 0.05,\n",
    "                            'rotation_cycle_consistency': 1e-3,\n",
    "                            'translation_cycle_consistency': 5e-2,\n",
    "                            'depth_variance': 0.0,\n",
    "                            'motion_smoothing': 1.0,\n",
    "                            'motion_drift': 0.2,\n",
    "                        }\n",
    "        self.hparams = hparams\n",
    "        self.motion_field_burning_steps = 20000\n",
    "        self.depth_net = DispNetS()\n",
    "        intrinsics_mat = None\n",
    "        if self.hparams.intrinsics:\n",
    "            intrinsics_mat = np.loadtxt('./intrinsics.txt', delimiter=',')\n",
    "            intrinsics_mat = intrinsics_mat.reshape(3, 3)\n",
    "        self.object_motion_net = MotionVectorNet(auto_mask=True, \n",
    "                        intrinsics=self.hparams.intrinsics, intrinsics_mat=intrinsics_mat)\n",
    "        self.loss_func = DMPLoss(self.default_loss_weights)\n",
    "        self.delete_file = True\n",
    "        train_batches = len(self.train_dataloader())\n",
    "        \n",
    "        self.base_step = (train_batches) // self.hparams.accumulate_grad_batches\n",
    "        # torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        endpoints = self.forward(batch, batch_idx, train=False)\n",
    "        loss_val = self.loss_func(endpoints)\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "        outputs = OrderedDict({\n",
    "            'val_loss': loss_val,\n",
    "        })\n",
    "        return outputs\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        tqdm_dict = {}\n",
    "        for metric_name in [\"val_loss\"]:\n",
    "            metric_total = 0\n",
    "            for output in outputs:\n",
    "                metric_value = output[metric_name]\n",
    "                # reduce manually when using dp\n",
    "                if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                    metric_value = torch.mean(metric_value)\n",
    "                metric_total += metric_value\n",
    "            tqdm_dict[metric_name] = metric_total / len(outputs)\n",
    "\n",
    "        result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': tqdm_dict[\"val_loss\"]}\n",
    "        return result\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = DepthMotionDataset(mode='valid', transform=transforms.Compose([\n",
    "                                        transforms.Resize(size=rsize_factor),\n",
    "                                        transforms.ToTensor(),\n",
    "                                    ]),\n",
    "                                    root_dir='./',\n",
    "                                    )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                                        dataset=val_dataset,\n",
    "                                        batch_size=self.hparams.batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=8,\n",
    "                                        drop_last = False,\n",
    "                                        sampler=None,\n",
    "                                        pin_memory=False,\n",
    "                                    )\n",
    "        print (\"Total valid example : {}\".format((len(val_loader.dataset))))\n",
    "        return val_loader\n",
    "\n",
    "    @staticmethod\n",
    "def add_model_specific_args(parent_parser):\n",
    "    parser = argparse.ArgumentParser(parents=[parent_parser])\n",
    "        \n",
    "    parser.add_argument('--epochs', default=90, type=int, metavar='N',\n",
    "                            help='number of total epochs to run')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                            help='seed for initializing training. ')\n",
    "    parser.add_argument('-b', '--batch-size', default=8, type=int,\n",
    "                        metavar='N',\n",
    "                        help='mini-batch size (default: 256), this is the total batch size of all GPUs on the current node when using Data Parallel or Distributed Data Parallel')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float,\n",
    "                        metavar='LR', help='initial learning rate', dest='lr')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)',\n",
    "                        dest='weight_decay')\n",
    "    parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
    "                        help='use pre-trained model')\n",
    "    parser.add_argument('--intrinsics', dest='intrinsics', action='store_true',\n",
    "                        help='use specified intrinsics')\n",
    "    return parser\n",
    "\n",
    "def get_args():\n",
    "    parent_parser = argparse.ArgumentParser(add_help=False)\n",
    "    parent_parser.add_argument('--gpus', type=int, default=0,\n",
    "                               help='how many gpus')\n",
    "    parent_parser.add_argument('--distributed-backend', type=str, default='dp', choices=('dp', 'ddp', 'ddp2'),\n",
    "                               help='supports three options dp, ddp, ddp2')\n",
    "    parent_parser.add_argument('--use-16bit', dest='use_16bit', action='store_true',\n",
    "                               help='if true uses 16 bit precision')\n",
    "    parent_parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                               help='evaluate model on validation set')\n",
    "    parent_parser.add_argument('-cf', '--clear-folder', dest='clear_folder', action='store_true',\n",
    "                               help='clear the folder')\n",
    "    parent_parser.add_argument('-agb', '--accumulate-grad-batches', dest='accumulate_grad_batches',type=int,\n",
    "                                default=4)\n",
    "\n",
    "    parser = DepthMotionLightningModel.add_model_specific_args(parent_parser)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d19cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import intrinsics_utils\n",
    "from loss_fn import DMPLoss\n",
    "from depth_prediction_net import DispNetS\n",
    "from object_motion_net import MotionVectorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_ego_motion(rot, trans):\n",
    "    \"\"\"\n",
    "        Infer ego motion (pose) using rot and trans matrix.\n",
    "        Args:\n",
    "            rot : rotation matrix.\n",
    "            trans : translational matrix.\n",
    "        Returns :\n",
    "            avg_rot : rotation matrix for trajectory in world co-ordinates system.\n",
    "            avg_trans : translation matrix for trajectory in world co-ordinates system.\n",
    "    \"\"\"\n",
    "    rot12, rot21 = rot\n",
    "    rot12 = matrix_from_angles(rot12)\n",
    "    rot21 = matrix_from_angles(rot21)\n",
    "    trans12, trans21 = trans\n",
    "\n",
    "    avg_rot = 0.5 * (torch.linalg.inv(rot21) + rot12)\n",
    "    avg_trans = 0.5 * (-torch.squeeze(\n",
    "        torch.matmul(rot12, torch.unsqueeze(trans21, -1)), dim=-1) + trans12)\n",
    "    return avg_rot, avg_trans\n",
    "\n",
    "transform=transforms.Compose([transforms.Resize(size=(128,416)),\n",
    "                              transforms.ToTensor(),\n",
    "                             ])\n",
    "trajectory, positions = [], []\n",
    "position = np.zeros(3)\n",
    "orientation = np.eye(3)\n",
    "    \n",
    "    # Model Architecture\n",
    "if args.gpus != -1:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "depth_net = DispNetS()\n",
    "object_motion_net = MotionVectorNet(auto_mask=True, intrinsics=args.intrinsics, intrinsics_mat=intrinsics_mat)\n",
    "# Load Model\n",
    "# model = torch.load(args.model_path, map_location=device)[\"state_dict\"]\n",
    "# depth_model = { k.replace(\"depth_net.\", \"\") : v for k, v in model.items() if \"depth\" in k}\n",
    "# depth_net.load_state_dict(depth_model)\n",
    "# object_model = { k.replace(\"object_motion_net.\", \"\") : v for k, v in model.items() if \"object\" in k}\n",
    "# object_motion_net.load_state_dict(object_model)\n",
    "depth_net.eval()\n",
    "object_motion_net.eval()\n",
    "\n",
    "sample_a = transform(sample_a)\n",
    "sample_b = transform(sample_b)\n",
    "\n",
    "endpoints = {}\n",
    "rgb_seq_images = [sample_a.unsqueeze(0), sample_b.unsqueeze(0)]\n",
    "rgb_images = torch.cat((rgb_seq_images[0], rgb_seq_images[1]), dim=0)\n",
    "        \n",
    "depth_images = depth_net(rgb_images)\n",
    "depth_seq_images = torch.split(depth_images, depth_images.shape[0] // 2, dim=0)\n",
    "        \n",
    "endpoints['predicted_depth'] = depth_seq_images\n",
    "endpoints['rgb'] = rgb_seq_images\n",
    "motion_features = [torch.cat((endpoints['rgb'][0], endpoints['predicted_depth'][0]), dim=1),\n",
    "                   torch.cat((endpoints['rgb'][1], endpoints['predicted_depth'][1]), dim=1)]\n",
    "motion_features_stack = torch.cat(motion_features, dim=0)\n",
    "flipped_motion_features_stack = torch.cat(motion_features[::-1], dim=0)\n",
    "pairs = torch.cat([motion_features_stack, flipped_motion_features_stack], dim=1)\n",
    "        \n",
    "rot, trans, residual_translation, intrinsics_mat = object_motion_net(pairs)\n",
    "endpoints['residual_translation'] = torch.split(residual_translation, residual_translation.shape[0] // 2, dim=0)\n",
    "endpoints['background_translation'] = torch.split(trans, trans.shape[0] // 2, dim=0)\n",
    "endpoints['rotation'] = torch.split(rot, rot.shape[0] // 2, dim=0)\n",
    "intrinsics_mat = 0.5 * sum(torch.split(intrinsics_mat, intrinsics_mat.shape[0] // 2, dim=0))\n",
    "endpoints['intrinsics_mat'] = [intrinsics_mat] * 2\n",
    "endpoints['intrinsics_mat_inv'] = [intrinsics_utils.invert_intrinsics_matrix(intrinsics_mat)] * 2\n",
    "\n",
    "rot, trans = infer_ego_motion(endpoints['rotation'], endpoints['background_translation'])\n",
    "rot_angles = angles_from_matrix(rot).detach().cpu().numpy()\n",
    "rot, trans = rot.detach().cpu().numpy(), trans.detach().cpu().numpy()\n",
    "orientation = np.dot(orientation, rot[0])\n",
    "trajectory.append(np.concatenate((np.concatenate((orientation, trans.T), axis=1), [[0, 0, 0, 1]]), axis=0))\n",
    "position += np.dot(orientation, trans[0])\n",
    "positions.append(position)\n",
    "trajectory = np.vstack(trajectory) # Trajectories - 4x4 Pose matrix will be stored in [(N-1)*4,4] vector in trajectory.txt\n",
    "positions = np.array(positions) # Positions - 1x3 will be stored as [(N-1),3] vector in positions.txt\n",
    "np.savetxt('./trajectory.txt', trajectory)\n",
    "np.savetxt('./positions.txt', positions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
