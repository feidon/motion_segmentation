{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f3c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import intrinsics_utils\n",
    "from loss_fn import DMPLoss\n",
    "from depth_prediction_net import DispNetS\n",
    "from object_motion_net import MotionVectorNet\n",
    "\n",
    "rsize_factor = (128,416)\n",
    "\n",
    "class DepthMotionDataset(Dataset):\n",
    "    def __init__(self, mode='train', transform=None, root_dir='./',):\n",
    "        self.image_list = sorted(os.listdir(f'{root_dir}/images/taichung/'))[15000:16001]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_a, img_b = Image.open(f'{self.root_dir}/images/taichung/' + self.image_list[idx]), Image.open(f'{self.root_dir}/images/taichung/' + self.image_list[idx + 1])\n",
    "        if self.transform:\n",
    "            sample_a = self.transform(img_a)\n",
    "            sample_b = self.transform(img_b)\n",
    "        return [sample_a, sample_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89da8b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:35<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01/30, loss : 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:36<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 02/30, loss : 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:37<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 03/30, loss : 0.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:43<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 04/30, loss : 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:44<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 05/30, loss : 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:44<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 06/30, loss : 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:44<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 07/30, loss : 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:45<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 08/30, loss : 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:44<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 09/30, loss : 0.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:44<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10/30, loss : 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:44<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11/30, loss : 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:44<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12/30, loss : 0.019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [01:44<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13/30, loss : 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▎                                                        | 20/63 [00:35<01:15,  1.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2332651/2287095512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mendpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intrinsics_mat_inv'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mintrinsics_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert_intrinsics_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintrinsics_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/pytorch_depth_and_motion_planning/loss_fn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, endpoints)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# demand consistency, we need to `flip` `predicted_depth` as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             loss_endpoints = (\n\u001b[0;32m--> 225\u001b[0;31m                 consistency_losses.rgbd_and_motion_consistency_loss(\n\u001b[0m\u001b[1;32m    226\u001b[0m                     \u001b[0mtransformed_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0mrgb_stack\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/pytorch_depth_and_motion_planning/consistency_losses.py\u001b[0m in \u001b[0;36mrgbd_and_motion_consistency_loss\u001b[0;34m(frame1transformed_depth, frame1rgb, frame2depth, frame2rgb, rotation1, translation1, rotation2, translation2, validity_mask)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                      validity_mask=None):\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"A helper that bundles rgbd and motion consistency losses together.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     endpoints = rgbd_consistency_loss(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mframe1transformed_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mframe1rgb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/pytorch_depth_and_motion_planning/consistency_losses.py\u001b[0m in \u001b[0;36mrgbd_consistency_loss\u001b[0;34m(frame1transformed_depth, frame1rgb, frame2depth, frame2rgb, validity_mask)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# than hard coded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     ssim_error_mean = torch.mean(\n\u001b[0;32m--> 125\u001b[0;31m         multiply_no_nan(ssim_error, avg_weight)) # TODO\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     endpoints = {\n",
      "\u001b[0;32m/storage/pytorch_depth_and_motion_planning/consistency_losses.py\u001b[0m in \u001b[0;36mmultiply_no_nan\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmultiply_no_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 100\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "cudnn.deterministic = True\n",
    "PATH = './checkpoints/'\n",
    "\n",
    "default_loss_weights = {'rgb_consistency': 1.0,\n",
    "                        'ssim': 3.0,\n",
    "                        'depth_consistency': 0.05,\n",
    "                        'depth_smoothing': 0.05,\n",
    "                        'rotation_cycle_consistency': 1e-3,\n",
    "                        'translation_cycle_consistency': 5e-2,\n",
    "                        'depth_variance': 0.0,\n",
    "                        'motion_smoothing': 1.0,\n",
    "                        'motion_drift': 0.2,\n",
    "                       }\n",
    "batch_size = 16\n",
    "motion_field_burning_steps = 20000\n",
    "epochs = 30 #90\n",
    "intrinsics_mat = None\n",
    "use_intrinsics = False\n",
    "delete_file = True\n",
    "accumulate_grad_batches = 4\n",
    "metrics = 0\n",
    "\n",
    "train_dataset = DepthMotionDataset(mode='train', transform=transforms.Compose([transforms.Resize(size=rsize_factor),\n",
    "                                                                               transforms.ToTensor(),]),\n",
    "                                   root_dir='./')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=8,\n",
    "                                           drop_last = False,\n",
    "                                           sampler=None,\n",
    "                                           pin_memory=False,\n",
    "                                          )\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "depth_net = DispNetS()\n",
    "#depth_net = torch.nn.DataParallel(depth_net)\n",
    "depth_net.to(device)\n",
    "object_motion_net = MotionVectorNet(auto_mask=True, intrinsics=use_intrinsics, intrinsics_mat=intrinsics_mat).to(device)\n",
    "#object_motion_net = torch.nn.DataParallel(object_motion_net)\n",
    "object_motion_net.to(device)\n",
    "\n",
    "loss_func = DMPLoss(default_loss_weights)\n",
    "train_batches = len(train_loader)\n",
    "base_step = (train_batches) // accumulate_grad_batches\n",
    "\n",
    "optimizer = optim.Adam(list(depth_net.parameters()) + list(object_motion_net.parameters()), lr=1e-4, weight_decay=1e-4)\n",
    "#scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for rgb_seq_images in tqdm(train_loader):\n",
    "        depth_net.train()\n",
    "        object_motion_net.train()\n",
    "        \n",
    "        rgb_seq_images[0], rgb_seq_images[1] = rgb_seq_images[0].to(device), rgb_seq_images[1].to(device)\n",
    "        endpoints = {}\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        rgb_images = torch.cat((rgb_seq_images[0], rgb_seq_images[1]), dim=0)\n",
    "        depth_images = depth_net(rgb_images)\n",
    "        depth_seq_images = torch.split(depth_images, depth_images.shape[0] // 2, dim=0)\n",
    "        endpoints['predicted_depth'] = depth_seq_images\n",
    "        endpoints['rgb'] = rgb_seq_images\n",
    "        motion_features = [torch.cat((endpoints['rgb'][0], endpoints['predicted_depth'][0]), dim=1),\n",
    "                           torch.cat((endpoints['rgb'][1], endpoints['predicted_depth'][1]), dim=1)]\n",
    "        motion_features_stack = torch.cat(motion_features, dim=0)\n",
    "        flipped_motion_features_stack = torch.cat(motion_features[::-1], dim=0)\n",
    "        pairs = torch.cat([motion_features_stack, flipped_motion_features_stack], dim=1)\n",
    "        rot, trans, residual_translation, intrinsics_mat = object_motion_net(pairs)\n",
    "        if motion_field_burning_steps > 0.0:\n",
    "            step = base_step * epoch\n",
    "            step = torch.tensor(step).type(torch.FloatTensor)\n",
    "            burnin_steps = torch.tensor(motion_field_burning_steps).type(torch.FloatTensor)\n",
    "            residual_translation *= torch.clamp(2 * step / burnin_steps - 1, 0.0, 1.0)\n",
    "        endpoints['residual_translation'] = torch.split(residual_translation, residual_translation.shape[0] // 2, dim=0)\n",
    "        endpoints['background_translation'] = torch.split(trans, trans.shape[0] // 2, dim=0)\n",
    "        endpoints['rotation'] = torch.split(rot, rot.shape[0] // 2, dim=0)\n",
    "        intrinsics_mat = 0.5 * sum(torch.split(intrinsics_mat, intrinsics_mat.shape[0] // 2, dim=0))\n",
    "        endpoints['intrinsics_mat'] = [intrinsics_mat] * 2\n",
    "        endpoints['intrinsics_mat_inv'] = [intrinsics_utils.invert_intrinsics_matrix(intrinsics_mat)] * 2\n",
    "        \n",
    "        loss_val = loss_func(endpoints)\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "    #scheduler.step(metrics)\n",
    "    print(f'Epoch : {epoch + 1:02d}/{epochs}, loss : {loss_val:.03f}')\n",
    "    imageio.imwrite(f'./train_log/{epoch + 1:02d}.png',\n",
    "                    (endpoints['residual_translation'][1][-1].cpu().detach().numpy() * 255).astype(np.uint8))\n",
    "    torch.save(depth_net.state_dict(), PATH + 'depth_model.ckpt')\n",
    "    torch.save(object_motion_net.state_dict(), PATH + 'object_motion_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "132c8fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "        [-0., -0., -0.,  ..., -0., -0., -0.]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints['residual_translation'][0][1, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91b95267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints['background_translation'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cal(h, w, pad, ker, stri, dila=1):\n",
    "    return math.floor(((h + 2 * pad - dila * (ker - 1) -1) / stri) + 1), math.floor(((w + 2 * pad - dila * (ker - 1) -1) / stri) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal(32, 104, 0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eab449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcal(h, w, pad, ker, stri, dila=1):\n",
    "    return (h - 1) * stri - 2 * pad + ker, (w - 1) * stri - 2 * pad + ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcal(32, 104, 1, 1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
